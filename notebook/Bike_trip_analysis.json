{
	"name": "Bike_trip_analysis",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkprocess",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1",
				"spark.autotune.trackingId": "760c36d3-6fed-4a05-b56d-1d81378f0888"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/aa039b5d-32d4-4dcb-a412-fe8fbac60a50/resourceGroups/decourse/providers/Microsoft.Synapse/workspaces/divvy-wh/bigDataPools/sparkprocess",
				"name": "sparkprocess",
				"type": "Spark",
				"endpoint": "https://divvy-wh.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkprocess",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					},
					"collapsed": false
				},
				"source": [
					"%%pyspark\r\n",
					"df = spark.read.load('abfss://bike-data@azradls.dfs.core.windows.net/historical_data/202111-divvy-tripdata.csv', format='csv'\r\n",
					"## Append the monthly data to table 'trip_data'\r\n",
					", header=True\r\n",
					")\r\n",
					"display(df.limit(10))   \r\n",
					"\r\n",
					"#save the df to trip_data\r\n",
					"\r\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					},
					"collapsed": false
				},
				"source": [
					"%%pyspark\r\n",
					"df = spark.read.option(\"multiline\",\"true\").json('abfss://bike-data@azradls.dfs.core.windows.net/station_description.json')\r\n",
					"## If header exists uncomment line below\r\n",
					"\r\n",
					"#display(df.limit(10))   \r\n",
					"print(df.select('data.stations[0]').collect())\r\n",
					"\r\n",
					""
				],
				"execution_count": 93
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"#from pyspark.sql.types import MapType,StringType\r\n",
					"#from pyspark.sql.functions import from_json, json_tuple\r\n",
					"from pyspark.sql.functions import explode, col, array\r\n",
					"\r\n",
					"\r\n",
					"df = spark.read.option(\"multiline\",\"true\").json('abfss://bike-data@azradls.dfs.core.windows.net/station_description.json')\r\n",
					"#df.printSchema()\r\n",
					"\r\n",
					"data = df.select(explode(array(\"data\")).alias(\"data\"))\r\n",
					"#data.show()\r\n",
					"\r\n",
					"stations= data.select(explode(array(\"data.stations\")).alias(\"stations\"))\r\n",
					"#stations.printSchema()\r\n",
					"#stations.show()\r\n",
					"\r\n",
					"stations2= stations.select(explode(\"stations\").alias(\"stations2\"))\r\n",
					"stations2.printSchema()\r\n",
					"station_description= station2.select(\"station2.name\", \"station2.station_id\", \"station2.capacity\", \"station2.short_name\", \"station2.dockless_bikes_capacity\", \"station2.station_type\", \"station2.lat\",\"station2.lon\",).show(5)\r\n",
					"station_description.write('overwrite').parquet(\"station_description\")\r\n",
					"\r\n",
					"#stations2.select('stations2'.getItem(0).as('col1'),'stations2'.getItem(1).as('col2'),'stations2'.getItem(2).as('col3')).show()\r\n",
					"#stations3=stations2.withColumn('stations2', 'stations2'.getItem(0))\r\n",
					"#stations3.show()\r\n",
					"#df.withColumn(\"item\", df[\"item\"][0])\r\n",
					"\r\n",
					"\r\n",
					"#df2=df.withColumn(\"data\",from_json(df.data,[MapType(StringType(),StringType()))])\r\n",
					"#df2.printSchema()\r\n",
					"#df2.show(truncate=False)\r\n",
					"\r\n",
					"#with open(\"dataflow_schema.json\", \"w\") as fp:\r\n",
					" #   fp.write(df.schema.json())\r\n",
					""
				],
				"execution_count": 110
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import  date_trunc, date_format, datediff,unix_timestamp, col, desc\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"df_calendar = spark.read.load('abfss://bike-data@azradls.dfs.core.windows.net/calendar_table.csv', format='csv'\r\n",
					"## Append the monthly data to table 'trip_data'\r\n",
					", header=True\r\n",
					")\r\n",
					"\r\n",
					"df_trip = spark.read.load('abfss://bike-data@azradls.dfs.core.windows.net/historical_data/202111-divvy-tripdata.csv', format='csv'\r\n",
					"## Append the monthly data to table 'trip_data'\r\n",
					", header=True\r\n",
					")\r\n",
					"\r\n",
					"df_trip_2=df_trip\\\r\n",
					".select(\"rideable_type\", \"start_station_name\",\"end_station_name\",\"rideable_type\",\"started_at\", \"ended_at\" )\\\r\n",
					".withColumn(\"started_date\", date_trunc(\"Day\", \"started_at\") ) \\\r\n",
					".withColumn(\"ended_date\", date_trunc(\"Day\", \"ended_at\") ) \\\r\n",
					".withColumn(\"started_hour\", date_trunc(\"Hour\", \"started_at\") ) \\\r\n",
					".withColumn(\"ended_hour\", date_trunc(\"Hour\", \"ended_at\") ) \\\r\n",
					".withColumn(\"started_hour\",date_format(\"started_at\", 'k ')) \\\r\n",
					".withColumn(\"ended_hour\",date_format(\"started_at\", 'k'))\\\r\n",
					".withColumn(\"trip_duration\",unix_timestamp(\"ended_at\")-unix_timestamp(\"started_at\"))\\\r\n",
					".filter(col(\"start_station_name\") != \"undefined\" )\\\r\n",
					".filter(col(\"end_station_name\") != \"undefined\" )\\\r\n",
					"\r\n",
					"#.groupBy(\"started_hour\").count\r\n",
					"\r\n",
					"display(df_trip_2.limit(2))\r\n",
					"# Starting station with longest trip duration\r\n",
					"#display(df_trip_2.groupBy(\"start_station_name\").sum(\"trip_duration\"))\r\n",
					"\r\n",
					"# Start Station with most rides\r\n",
					"df_trip_3 = df_trip_2.groupBy(\"start_station_name\",\"started_hour\").count().orderBy(desc(\"count\"),desc(\"start_station_name\"))\r\n",
					"\r\n",
					"df_trip_3_reordered = df_trip_3.groupBy(\"start_station_name\").pivot(\"started_hour\").sum()\r\n",
					"\r\n",
					"display(df_trip_3_reordered.select(sorted(df_trip_3_reordered.columns,reverse=True)))\r\n",
					"\r\n",
					"df_stations=spark.read.table(\"station_description\")\r\n",
					"station_metrics = df_trip_3_reordered.join(df_stations, df_trip_3_reordered.start_station_name==df_stations.name,\"inner\")\r\n",
					"station_metrics.write.mode(\"Overwrite\").json(\"station_metrics\")\r\n",
					"#print(type(station_metrics))\r\n",
					"#('overwrite').parquet(\"station_metrics\")\r\n",
					"\r\n",
					"#display(df_basket_reordered)\r\n",
					"\r\n",
					"\r\n",
					"#display(df_trip.join(df_calendar, date_trunc(df_trip.started_at, \"dt\") == df_calendar.Date,\"inner\" ))\r\n",
					"\r\n",
					"#df_calendar.createOrReplaceTempView(\"calendar\")\r\n",
					"\r\n",
					"#spark.table(\"calendar\").count "
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df_calendar = spark.read.load('abfss://bike-data@azradls.dfs.core.windows.net/calendar_table.csv', format='csv'\r\n",
					"## Append the monthly data to table 'trip_data'\r\n",
					", header=True\r\n",
					")\r\n",
					"\r\n",
					"\r\n",
					"display(df_calendar.limit(2))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import  date_trunc, from_unixtime, row_number, col, struct\r\n",
					"from pyspark.sql.window import Window\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"dfStream = spark.read\\\r\n",
					"    .format(\"cosmos.oltp\")\\\r\n",
					"    .option(\"spark.synapse.linkedService\", \"DivvyCosmosDB\")\\\r\n",
					"    .option(\"spark.cosmos.container\", \"station_status\")\\\r\n",
					"    .option(\"spark.cosmos.changeFeed.readEnabled\", \"true\")\\\r\n",
					"    .option(\"spark.cosmos.changeFeed.startFromTheBeginning\", \"true\")\\\r\n",
					"    .option(\"spark.cosmos.changeFeed.checkpointLocation\", \"/localReadCheckpointFolder\")\\\r\n",
					"    .option(\"spark.cosmos.changeFeed.queryName\", \"streamQuery\")\\\r\n",
					"    .load()\r\n",
					"\r\n",
					"df_station_metrics = spark.read.json(\"station_metrics\")\r\n",
					"#display(df_station_metrics.limit(5))\r\n",
					"\r\n",
					"df_station_metrics_trend= df_station_metrics.withColumn(\"trend\", struct(col(\"1 \"), col(\"2 \"), col(\"3 \"), col(\"4 \"), col(\"5 \"), col(\"6 \"), col(\"7 \"), col(\"8 \"), col(\"9 \"), col(\"10 \"), col(\"11 \"), col(\"12 \")\\\r\n",
					",col(\"13 \"), col(\"14 \"), col(\"15 \"), col(\"16 \"), col(\"17 \"), col(\"18 \"), col(\"19 \"), col(\"20 \"), col(\"21 \"), col(\"22 \"), col(\"23 \"), col(\"24 \")))\\\r\n",
					".drop(\"1 \",\"2 \",\"3 \",\"4 \",\"5 \",\"6 \",\"7 \",\"8 \",\"9 \",\"10 \",\"11 \",\"12 \"\\\r\n",
					",\"13 \",\"14 \",\"15 \",\"16 \",\"17 \",\"18 \",\"19 \",\"20 \",\"21 \",\"22 \",\"23 \",\"24 \")\r\n",
					"\r\n",
					"\r\n",
					"windowSpec  = Window.partitionBy(\"id\").orderBy(col(\"last_updated\").desc())\r\n",
					"\r\n",
					"df_ranked = dfStream.join(df_station_metrics_trend, dfStream.station_id == df_station_metrics_trend.station_id , \"inner\")\\\r\n",
					".withColumn(\"real_time_stamp\",from_unixtime(\"_ts\") )\\\r\n",
					".withColumn(\"last_reported_time_stamp\",from_unixtime(\"last_reported\") )\\\r\n",
					".withColumn(\"last_updated_time_stamp\",from_unixtime(\"last_updated\") ) \\\r\n",
					".withColumn(\"row_number\",row_number().over(windowSpec)) \\\r\n",
					".drop(dfStream.station_id)\\\r\n",
					".drop(\"_attachments\",\"_etag\",\"_rid\",\"_self\",\"_ts\",\"id\",\"last_reported\",\"last_updated\")\\\r\n",
					".where(col(\"row_number\")==1)\r\n",
					"\r\n",
					"#display(df_ranked)\r\n",
					"#.withColumn(\"row_number\",row_number().over(Window.partitionBy(df_station_metrics.station_id).orderBy(\"real_time_stamp\"))).filter(col(\"row_number\")==1)\r\n",
					"#.filter(df_station_metrics.start_station_name == \"Clark St & Grace St\")\\\r\n",
					"#.filter(dfStream.station_id == \"165\")\\\r\n",
					"# .where((dfStream.station_id == '165'))\\\r\n",
					"\r\n",
					"df_ranked.coalesce(1).write.mode(\"Overwrite\").json(\"station_metrics_live\")\r\n",
					""
				],
				"execution_count": 90
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import  date_trunc, from_unixtime, row_number, col\r\n",
					"from pyspark.sql.window import Window\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"dfStream = spark.read\\\r\n",
					"    .format(\"cosmos.oltp\")\\\r\n",
					"    .option(\"spark.synapse.linkedService\", \"DivvyCosmosDB\")\\\r\n",
					"    .option(\"spark.cosmos.container\", \"station_status\")\\\r\n",
					"    .option(\"spark.cosmos.changeFeed.readEnabled\", \"true\")\\\r\n",
					"    .option(\"spark.cosmos.changeFeed.startFromTheBeginning\", \"true\")\\\r\n",
					"    .option(\"spark.cosmos.changeFeed.checkpointLocation\", \"/localReadCheckpointFolder\")\\\r\n",
					"    .option(\"spark.cosmos.changeFeed.queryName\", \"streamQuery\")\\\r\n",
					"    .load()\r\n",
					"\r\n",
					"#display(dfStream.filter(dfStream.station_id == \"165\"))\r\n",
					"  # )\r\n",
					"  #.withColumn(\"row_number\",row_number().over(Window.partitionBy(df_station_metrics.station_id).orderBy(\"last_updated\")))\\\r\n",
					"\r\n",
					"windowSpec  = Window.partitionBy(\"id\").orderBy(col(\"last_updated\").desc())\r\n",
					"df_ranked = dfStream.withColumn(\"row_number\",row_number().over(windowSpec)) #.where((dfStream.station_id == '165'))\r\n",
					"display(df_ranked.where(col(\"row_number\")==1))\r\n",
					"\r\n",
					""
				],
				"execution_count": 55
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"\r\n",
					"from pyspark.sql.functions import  struct, col\r\n",
					"import pandas as pd\r\n",
					"\r\n",
					"df = spark.read.json(\"station_metrics\")\r\n",
					"#df.withColumn()\r\n",
					"#display(df_station_metrics)\r\n",
					"dummy_df = pd.DataFrame()\r\n",
					"\r\n",
					"\r\n",
					"#display(df.withColumn(\"trend\", struct(col(\"1 \"), col(\"2 \"), col(\"3 \"), col(\"4 \"), col(\"5 \"), col(\"6 \"), col(\"7 \"), col(\"8 \"), col(\"9 \"), col(\"10 \"), col(\"11 \"), col(\"12 \")\\\r\n",
					"#,col(\"13 \"), col(\"14 \"), col(\"15 \"), col(\"16 \"), col(\"17 \"), col(\"18 \"), col(\"19 \"), col(\"20 \"), col(\"21 \"), col(\"22 \"), col(\"23 \"), col(\"24 \")))\r\n",
					"#.drop(\"1 \",\"2 \",\"3 \",\"4 \",\"5 \",\"6 \",\"7 \",\"8 \",\"9 \",\"10 \",\"11 \",\"12 \"\\\r\n",
					"#,\"13 \",\"14 \",\"15 \",\"16 \",\"17 \",\"18 \",\"19 \",\"20 \",\"21 \",\"22 \",\"23 \",\"24 \")\r\n",
					"#)\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"execution_count": 97
			}
		]
	}
}